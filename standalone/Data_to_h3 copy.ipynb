{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import h3\n",
    "from shapely.geometry import Point, shape\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls(base_url):\n",
    "    \"\"\"\n",
    "    Scrapes all URLs from a webpage.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The URL of the webpage to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs found on the webpage.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        urls = [a['href'] for a in soup.find_all('a', href=True) if 'cbs_vk100' in a['href']]\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {base_url} due to {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gpkg(gpkg_path, shapefile_path, output_path):\n",
    "    \"\"\"\n",
    "    Clips a GeoPackage to the extent of a shapefile and saves the result as a GeoJSON.\n",
    "\n",
    "    Args:\n",
    "        gpkg_path (str): Path to the GeoPackage.\n",
    "        shapefile_path (str): Path to the shapefile.\n",
    "        output_path (str): Path to save the clipped GeoJSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gdf = gpd.read_file(gpkg_path)\n",
    "        gdf = gdf.to_crs(epsg=4326)\n",
    "        clip_gdf = gpd.read_file(shapefile_path)\n",
    "        clip_gdf = clip_gdf.to_crs(epsg=4326)\n",
    "        clipped = gpd.clip(gdf, clip_gdf)\n",
    "        clipped.to_file(output_path, driver='GeoJSON')\n",
    "    except Exception as e:\n",
    "        print(f\"Error clipping GeoPackage: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract(url, year, download_path, extract_path):\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from a URL and extracts its contents.\n",
    "\n",
    "    Args:\n",
    "        year (int): The year of the data.\n",
    "        download_path (str): Path to save the downloaded ZIP file.\n",
    "        extract_path (str): Path to extract the contents of the ZIP file.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file was successfully downloaded and extracted, False otherwise.\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(download_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading or extracting file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_feature(feature, resolution):\n",
    "  \"\"\"\n",
    "  Processes a single GeoDataFrame feature and converts geometry to H3 cell.\n",
    "\n",
    "  Args:\n",
    "    feature (tuple): A single row from the DataFrame, represented as a tuple.\n",
    "    resolution (int): H3 resolution (0-17).\n",
    "\n",
    "  Returns:\n",
    "    tuple: A tuple containing the H3 cell and population data.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    aantal_inwoners, geometry = feature\n",
    "    centroid = geometry.centroid\n",
    "    h3_cell = h3.geo_to_h3(centroid.y, centroid.x, resolution)\n",
    "    return h3_cell, aantal_inwoners\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing feature: {e}\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_geojson(clip_gdf, resolution, output_filepath):\n",
    "  \"\"\"\n",
    "  Processes a GeoJSON file containing population data and aggregates it to H3 cells.\n",
    "\n",
    "  Args:\n",
    "    clip_gdf (str): URL or path to the GeoJSON file.\n",
    "    resolution (int): H3 resolution (0-17).\n",
    "    output_filepath (str): Path to save the resulting CSV file.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    gdf = gpd.read_file(clip_gdf)\n",
    "    gdf = gdf[['aantal_woningen', 'geometry']]\n",
    "    gdf = gdf[gdf['aantal_woningen'] > 0]  # Filter out rows with a value of 0\n",
    "    n_cores = os.cpu_count()\n",
    "    with Pool(processes=n_cores) as pool:\n",
    "      # Convert DataFrame to list of tuples\n",
    "      data = list(gdf.itertuples(index=False, name=None))\n",
    "      results = pool.starmap(process_feature, zip(data, [resolution] * len(gdf)))\n",
    "    h3_data, population_data = zip(*results)\n",
    "    df = pd.DataFrame({'hex9': h3_data, 'value': population_data})\n",
    "    df.to_csv(output_filepath, index=False)\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    current_year = datetime.datetime.now().year -1\n",
    "    base_url = 'https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/geografische-data/kaart-van-100-meter-bij-100-meter-met-statistieken#:~:text=In%20deze%20kaart%20met%20vierkanten,en%20nabijheid%20van%20voorzieningen%20samengesteld.'\n",
    "    urls = scrape_urls(base_url)\n",
    "    for year in range(current_year - 1, current_year - 12, -1):\n",
    "        download_path = f'./cbs_{year}.zip'\n",
    "        extract_path = f'./cbs_{year}'\n",
    "        url_gpkg = os.path.join(extract_path, f'cbs_vk100_{year}_v1.gpkg')\n",
    "        output_filepath = f'./cbs2_{year}_h3.csv'\n",
    "        url = next((u for u in urls if str(year) in u), None)\n",
    "        if url:\n",
    "            # if download_and_extract(url, year, download_path, extract_path):\n",
    "            #     clip_gpkg(url_gpkg, '../shapefiles/zh_poly.shp', './clipped.geojson')\n",
    "            process_geojson('./clipped.geojson', 9, output_filepath)\n",
    "            break\n",
    "        else:\n",
    "            print(f\"No URL available for {year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
